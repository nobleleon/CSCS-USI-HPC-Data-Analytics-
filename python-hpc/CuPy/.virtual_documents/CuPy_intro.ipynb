





import numpy as np
import cupy as cp
import matplotlib.pyplot as plt


from timers import cupy_timer, cpu_timer





A = cp.zeros((1000, 1000))
print(f'Array A is of type: {type(A)}')

B = cp.array([[1, 2, 3], 
              [4, 5, 6]])

print(B)


cp_rng = cp.random.default_rng()
B = cp_rng.uniform(size=(1000, 1000))
mu = B.mean()
print(f'The mean value is: {mu}')








np_rng = np.random.default_rng()
A_cpu = np_rng.uniform(size=(1000, 1000))

# Create CuPy array from NumPy array
A_gpu = cp.array(A_cpu)

mu_cpu = A_cpu.mean()
mu_gpu = A_gpu.mean()
print(f'Mean using CPU: {mu_cpu}')
print(f'Mean using GPU: {mu_gpu}')





x_gpu = cp.ones((1000, 1000))
x_cpu = x_gpu.get()
print(f'Type of CPU array is: {type(x_cpu)}')


y_gpu = cp_rng.uniform(size=(1000, 1000))
y_cpu = cp.asnumpy(y_gpu)
print(f'Type of CPU array is: {type(y_cpu)}')





z_gpu = cp_rng.uniform(size=(1000, 1000))
z_cpu = np.empty(shape=z_gpu.shape, dtype=z_gpu.dtype)
cp.asnumpy(z_gpu, out=z_cpu);








x_cpu = np_rng.random((2000, 2000))
y_cpu = np_rng.random((2000, 2000))
x_gpu = cp.array(x_cpu)
y_gpu = cp.array(y_cpu)

z_cpu = x_cpu @ y_cpu
z_gpu = x_gpu @ y_gpu








A_cpu = np_rng.random((10000, 10000))
b_cpu = np_rng.random(10000)
A_gpu = cp.array(A_cpu)
b_gpu = cp.array(b_cpu)

with cpu_timer(True):
    np.linalg.solve(A_cpu, b_cpu)
    
with cupy_timer(True):
    cp.linalg.solve(A_gpu, b_gpu)





def euclidean_distance_cpu(x, y):
    x2 = np.einsum('ij,ij->i', x, x)[:, np.newaxis]
    y2 = np.einsum('ij,ij->i', y, y)[np.newaxis, :]
    xy = x @ y.T
    return np.abs(x2 + y2 - 2.0 * xy)


def euclidean_distance_gpu(x, y):
    x2 = cp.einsum('ij,ij->i', x, x)[:, cp.newaxis]
    y2 = cp.einsum('ij,ij->i', y, y)[cp.newaxis, :]
    xy = x @ y.T
    return cp.abs(x2 + y2 - 2.0 * xy)


x_cpu = np_rng.random((5000, 5000))
y_cpu = np_rng.random((5000, 5000))
x_gpu = cp.array(x_cpu)
y_gpu = cp.array(y_cpu)

with cpu_timer():
    eu_cpu = euclidean_distance_cpu(x_cpu, y_cpu)

with cupy_timer():
    eu_gpu = euclidean_distance_gpu(x_gpu, y_gpu)
    
assert(np.allclose(eu_cpu, eu_gpu.get()))





def euclidean_distance(x, y):
    array_mod = cp.get_array_module(x)
    x2 = array_mod.einsum('ij,ij->i', x, x)[:, array_mod.newaxis]
    y2 = array_mod.einsum('ij,ij->i', y, y)[array_mod.newaxis, :]
    xy = x @ y.T
    return array_mod.abs(x2 + y2 - 2.0 * xy)


with cpu_timer():
    eu_cpu = euclidean_distance(x_cpu, y_cpu)

with cupy_timer():
    eu_gpu = euclidean_distance(x_gpu, y_gpu)
    
assert(np.allclose(eu_cpu, eu_gpu.get()))








from numba import prange, njit

@njit(parallel=True)
def julia_set_numba(X, Y, cx, cy, iter_max, radius2):
    ny = X.shape[0]
    nx = Y.shape[1]
    julia = np.zeros_like(X, dtype=np.int32)
    for i in prange(ny):
        for j in range(nx):
            x = X[i, j]
            y = Y[i, j]
            k = 0
            while x * x + y * y < radius2 and k < iter_max:
                x, y = x * x - y * y + cx, 2.0 * x * y + cy
                k = k + 1
                
            julia[i, j] = k 
            
    return julia


X, Y = np.meshgrid(np.linspace(-2.0 , 2.0, 5000), np.linspace(-2.0, 2.0, 5000))
fig = plt.figure(figsize=(10, 10))
ax = fig.add_subplot(111)
c = -0.9 + 0.22143j
radius2 = 4.0

with cpu_timer(True):
    julia = julia_set_numba(X, Y, c.real, c.imag, 100, radius2)

ax.set_aspect('equal')
ax.imshow(julia, extent=[-2, 2, -2, 2]);





julia_kernel = cp.ElementwiseKernel('float64 X, float64 Y, float64 cx, float64 cy, int32 itermax, float64 radius2',
                                    'int32 julia',
                                    f'''julia = 0;
                                    double x = X, y = Y;
                                    double xtemp;
                                    int nit = 0;
                                    while(x * x + y * y < radius2 && nit < itermax) {{
                                        xtemp = x * x - y * y + cx;
                                        y = 2.0 * x * y + cy;
                                        x = xtemp;
                                        nit += 1;
                                    }}
                                    julia = nit;''', 'julia_kernel')


X, Y = cp.meshgrid(cp.linspace(-2.0 , 2.0, 5000), cp.linspace(-2.0, 2.0, 5000))
fig = plt.figure(figsize=(10, 10))
ax = fig.add_subplot(111)
c = -0.9 + 0.22143j
with cupy_timer():
    julia_gpu = julia_kernel(X, Y, c.real, c.imag, 100, 4.0)
    julia_array = julia_gpu.get()
ax.set_aspect('equal')
ax.imshow(julia_array, extent=[-2, 2, -2, 2]);





julia_rawkernel = cp.RawKernel(r'''
        extern "C" 
        __global__ void julia_rawkernel(const double* X, const double* Y, const double cx,
                              const double cy, const int itermax, const int nx,
                              const int ny, const double radius2, int* julia)  {
        int tidx = blockDim.x * blockIdx.x + threadIdx.x;
        int tidy = blockDim.y * blockIdx.y + threadIdx.y;
        int niter = 0;
        double tempx;
        if(tidx < nx && tidy < ny) 
        {
            int tid = tidy * nx + tidx;
            double x = X[tid], y = Y[tid];
            while((x * x + y * y) < radius2 && niter < itermax) {
                tempx = x * x - y * y + cx;
                y = 2.0 * x * y + cy;
                x = tempx;
                niter +=1 ;
            }
            julia[tid] = niter;
        }
    }''', 'julia_rawkernel')


fig = plt.figure(figsize=(10, 10))
ax = fig.add_subplot(111)
c = -0.9 + 0.22143j
threads_x = 32
threads_y = 32
x_points = 5000
y_points = 5000
x_blocks = (x_points - 1) // threads_x + 1 
y_blocks = (y_points - 1) // threads_y + 1
X_gpu, Y_gpu = cp.meshgrid(cp.linspace(-2.0 , 2.0, x_points), cp.linspace(-2.0, 2.0, y_points))
with cupy_timer():
    julia_gpu = cp.zeros_like(X_gpu, dtype=cp.int32)
    julia_rawkernel((x_blocks, y_blocks), (threads_x, threads_y), (X_gpu, Y_gpu, c.real, c.imag, 100, 
                    x_points, y_points, 2.0 ** 2, julia_gpu))
    julia_array = julia_gpu.get()
ax.imshow(julia_array, extent=[-2, 2, -2, 2]);





from cupyx import jit

@jit.rawkernel()
def julia_kernel(X, Y, cx, cy, itermax, nx, ny, radius2, julia):
    tidx = jit.blockDim.x * jit.blockIdx.x + jit.threadIdx.x
    tidy = jit.blockDim.y * jit.blockIdx.y + jit.threadIdx.y
    x = X[tidy, tidx]
    y = Y[tidy, tidx]   
    niter = 0
    if tidx < nx and tidy < ny:
        while (x * x + y * y) < radius2 and niter < itermax:
            x, y = (x * x - y * y + cx), (2.0 * x * y + cy)
            niter += 1
            
        julia[tidy, tidx] = niter


fig = plt.figure(figsize=(10, 10))
ax = fig.add_subplot(111)
c = cp.complex64(-0.9 + 0.22143j)
threads_x = 32
threads_y = 32
x_points = 5000
y_points = 5000
x_blocks = (x_points - 1) // threads_x + 1 
y_blocks = (y_points - 1) // threads_y + 1
X_gpu, Y_gpu = cp.meshgrid(cp.linspace(-2.0 , 2.0, x_points), cp.linspace(-2.0, 2.0, y_points))
radius = cp.float64(2.0)
with cupy_timer():
    julia_gpu = cp.zeros_like(X_gpu, dtype=cp.int32)
    julia_kernel[(x_blocks, y_blocks), (threads_x, threads_y)](
        X_gpu, Y_gpu, c.real, c.imag, 100, x_points, y_points, radius ** 2, julia_gpu
    )
    julia_array = julia_gpu.get()
    
ax.imshow(julia_array, extent=[-2, 2, -2, 2]);





print(julia_kernel.cached_code)





from numba import njit, prange

@njit(parallel=True)
def mandelbrot(X, Y, radius2, itermax):
    mandel = np.empty(shape=X.shape, dtype=np.int32)
    for i in prange(X.shape[0]):
        for j in range(X.shape[1]):
            it = 0
            cx = X[i, j]
            cy = Y[i, j]
            x = cx
            y = cy
            while x * x + y * y < radius2 and it < itermax:
                x, y = x * x - y * y + cx, 2.0 * x * y + cy
                it += 1
            mandel[i, j] = it
            
    return mandel


X, Y = np.meshgrid(np.linspace(-2.0, 1, 1000), np.linspace(-1.0, 1.0, 1000))

fig = plt.figure(figsize=(15, 10))
ax = fig.add_subplot(111)

m = mandelbrot(X, Y, 4.0, 100)
    
ax.imshow(np.log(1 + m), extent=[-2.0, 1, -1.0, 1.0]);
ax.set_aspect('equal')
ax.set_ylabel('Im[c]')
ax.set_xlabel('Re[c]');





def sigmoid_cpu(x):
    return 1.0 / (1.0 + np.exp(-x))

sigmoid_kernel = cp.ElementwiseKernel(
    'float64 X',
    'float64 sigmoid',
    'sigmoid = 1.0 / (1.0 + exp(-X));', 
    'sigmoid_kernel')

@cp.fuse
def sigmoid_fused(x):
    return 1.0 / (1.0 + cp.exp(-x))

def sigmoid_gpu(x):
    return 1.0 / (1.0 + cp.exp(-x))


x_cpu = np.linspace(-10.0, 10.0, 10_000_000)
x_gpu = cp.array(x_cpu)

print('Sigmoid Cpu: ', end='')
with cpu_timer(True):
    s_cpu = sigmoid_cpu(x_cpu)
    
print('Sigmoid Gpu: ', end='')
with cupy_timer(True):
    s_gpu = sigmoid_gpu(x_gpu)
    
print('Sigmoid Kernel: ', end='')
with cupy_timer(True):
    s_gpu_kernel = sigmoid_kernel(x_gpu)
    
print('Sigmoid Fused: ', end='')
with cupy_timer(True):
    s_gpu_fused = sigmoid_fused(x_gpu)
    
assert(np.allclose(s_cpu, s_gpu.get()))
assert(cp.allclose(s_cpu, s_gpu_kernel))
assert(cp.allclose(s_cpu, s_gpu_fused))

plt.close('all')
fig = plt.figure(figsize=(20, 10))
ax = fig.add_subplot(111)
ax.plot(x_cpu, s_cpu, '--');
ax.set_title('Sigmoid function', )
ax.set_xlabel('x')
ax.set_ylabel('y(x)')
ax.grid('Both');





import cupyx


device_array = cp.zeros(shape=(1000, 1000))
pinned_array = cupyx.zeros_like_pinned(device_array)








!nvidia-smi #


%whos ndarray


xx = cp.ones((10000, 100000))


mempool = cp.get_default_memory_pool()


used_gbs = mempool.used_bytes() / 1024 ** 3
total_gbs = mempool.total_bytes() / 1024 ** 3
print(f'Memory pool uses: {used_gbs:.3f} out of {total_gbs:.3f} GB')


del xx


mempool.free_all_blocks()


import gc


gc.collect()


#cp.cuda.set_allocator(None) # You can disable CuPy memory pool with this 
